from awsglue.dynamicframe import DynamicFrame
import requests 
import sys
from pyspark.sql import SparkSession 
from awsglue.transforms import * 
from awsglue.utils import getResolvedoptions 
from pyspark.context import SparkContext 
from pyspark.sql. functions import udf, col, lit
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, DoubleType, BooleanType, StringType, ArrayType, МарТуре
from awsglue.dynamicframe import DynamicFrame
frpm pyspark.sql.functions import to_timestamp, regexp_replace, when, concat, col, lit, udf

from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark import SparkConf 
# conf = (SparkConf ().set("spark.driver.maxResultSize", "4g"))

## @params: [JOB NAME]
#spark.conf.set("spark.driver.maxResultSize", "2g")
args = getResolvedOptions(sys.argv, ['JOB _NAME ' ])
# sc = SparkContext (conf = conf)
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB _NAME'], args)

# appname = args ['APP _NAME']
# batch size = int(args['BATCH SIZE'])
base_api_url = "https://developer.usastaffing.gov/"

api key = ' 6WsWZç1h11900R1JJEsNrVbQoCXyX5₫]lqvEmzbbvyR0Fadle08EYsAlIsmvWyr'


headers = {
    'Api-Key': api_Key
}



@udf(returnType=StringType())
def extract_id_from_links(links, rel_value):
    print("inside udf")
    for link in links:
        if link['rel'] == rel_ value:
            try:
                if ('/by/' in link['href']):
                     val = link['href'].split ('/')[-1]
                     res = val.split ('|')[-1]
                     return res
                 else:
                     val = linkl'hreE'］.split（/）［3］
                     res = val.split ('|')[-1]
                     return res
             except Exception as e:
                print (link['href"])
                print (e)
                return None
                
    return None


class ReadAPI:
   def__init__(self, table_name, api_url_key, table_schema, timestamp_col_1i):
      self.headers = headers
      self.table_schema = table_schema
      self.timestamp_columns = timestamp_col_li
      self.api_url = base_api_url+"/api/"+api_url_key
      self.table_name = table_name


   def convert_raw_list_to_df(self, raw_api_data_list):
       df = spark.createDataFrame(raw_api_data_list, schema=self.table_schema)
       return df
       
   def fetch_api_data (self,api_url):
       try：
           response = requests.get (api_url, headers=self. headers)
           if response. status_code == 200:
                return response.json()
       except Exception as e:
           print(e)
       return None 

   def handle_pagination(self, raw_api_data):
       page_dict = raw_api_data [paging']
       self.totalcount = page_dict['metadata']['totalCount']
       self.pagesize = page_dict[metadata']['pageSize']
       self.currentpage = page_dict['metadata']['currentPage']
       self.totalpages = page_dict['metadata'] [' totalPages ']
       self.snapshot = page_dict['metadata']['snapshot']
       self.previous = page_dict ['previous']
       self.next_api_url = page_dict['next']

   def extract_links_rel_list(self, raw_api_data):
       link li = raw_api_data['data'] [0]['_links']
       self.links_rel_list = [ele['rel'] for ele in link_li if ele['rel'] != 'self']

  def concat_df(self, new_df):
      self.df = self.df.union(new_df)
      # print("Concatanating with new df")
      
      
      

 def handle_links(self) :
     df = self.df
     print ("::: the rel list :::")
     print (self.links_rel_list)
     for rel_value in self. links_rel_list:
         #print (f"Extracting for : (rel value]")
         #df = df.withColumn(rel_value, lit(extract_id_ from_links(col("_links"),lit (rel_value))).cast(IntegerType ()))
         df = df.drop(rel_value)
    #print("converting json to text object")
    # df = df.withColumn('_links',col('_links').cast(StringType()))
  
    self.df = df
 
 def handle_timestamp_columns(self):
     for column in self.timestamp columns:
         self.df = self.df.withColumn(column, when(~col (column).contains('.'), concat(col(column) , lit('.000'))).otherwise(col (column)))
         self.df = self.df.withColumn(column, to_timestamp (col(column), "YYYY-MM-dd'T'HH:mm:ss.SSSSSSSSS"))


 def write_to_redshift(self):
     #self.df.printSchema()
     self.handle_links()
     self handle_ timestamp_columns()
     self.df = self.df.drop ('_links')
     print(f"Writing into redshift table: {my_conn_options}")
     self.df.printSchema()
     input_dynamic_frame = DynamicFrame.fromDF(self.df, glueContext, "input_dynamic_frame")
     glueContext.write_dynamic_frame.from_jdbc_conf(
         frame = input_dynamic_frame,
         catalog_connection= "Redshift connection_hcd-dev-db"
         connection_options = my_conn_options,
         redshift_tmp_dir ="s3://aws-glue-assets-094737541415-us-gov-west-1/temporary/")
     print(f"Loaded till - {self.currentpage}")
 
 def process_next_df(self) :
     raw_api_data = self.fetch_api_data(base_api_url+self.next_api_url)
     if(raw api_data) :
       self.handle_pagination(raw_api_data)
       raw_api_data_list = raw_api_data['data']
       new_df = self.convert_raw_list_to_df(raw_api_data_list)
       self.concat_df(new_df)
   
   
 def mark_checkpoint(self):
     print("-------------------------------------------------")
     print(f"----------------pageNo: {self.currentpage} pages processed-----------")
     print ("------------------------------------------------")
     self.df = spark.createDataFrame([], schema=self.table_schema)

 def mark_begining(self):
     print("=======================================================================")
     print(f"""=========================== {self.table_name} ================================
         totalCount = {self.totalcount｝
         pageSize = {self.pagesize}
         totalPages = {self.totalpages}
         snapshot = {self.snapshot}""")
     print("========================================================================")

 def execute(self):
     # first time operation
     raw_api_data = self.fetch_api_data(self.api_url)
     self.extract_links_rel_list(raw api data)
     if (raw_api_data):
         self.handle_pagination(raw_api_data)
         self.mark_begining()
         raw_api_data_list = raw_api_datal['data']
         self.df = self.convert_raw_list_to_df(raw_api data_list)
         if (self.totalpages==1):
             self.write_to_redshift（）
         for i in range (2,self.totalpages+1):
             if(i%batch _size==0):
                # append the data in the redshift
                self.process_next_df()
                self.write_to_redshift()
                    # empty the df
                # self.df = self.df.filter("1=0")
                self.mark_checkpoint()
                # begin again

                elif （i==self.totalpages）：
                     self.process_next_df()
                     self.write_to_redshift()
                else:
                    self.process_next_df()
         print("completed the process.")



import map_schema as mp

table_dict = {
    1: "Announcement",
    2: "Application",
    3: "Assessment",
    4: "CertificateApplication",
    5: "Certificate",
    6: "Customer",
    7: "NewHire",
    8: "NewHireAppointingAuthority",
    9: "Office",
    10: "OnboardingTask",
    11: "Organization",
    12: "RequestAppointingAuthority",
    13: "Request",
    14: "Review",
    15: "StaffingTask",
    16: "TimeToHire" ,
    17: "VacancyAppointingAuthority",
    18: "VacancyFlag" ,
    19: "Vacancy"}

process_tables = [4]
batch_size = 10

for i in process_tables:
    table_name = table_dict[i]
    redshift_table_name = f"prakash_csv_test.{table_name}"
    my_conn_options = {
    "dbtable" : redshift_table_name,
    "database": "hcd-dev-db"
        }
    table_schema, timestamp_col_li, api_url_key = mp. get_table attributes(table_name)
    api_reader = ReadAPI(redshift_ table_name, api_url_key, table_schema, timestamp col_ li)
    df = api_reader.execute()
