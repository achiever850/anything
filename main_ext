from awsglue.dynamicframe import DynamicFrame
import requests
import sys
from pyspark.sql import SparkSession
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from pyspark.sql.functions import udf, col, lit, when, to_timestamp
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark import SparkConf
import os
import json

# Configuration
args = getResolvedOptions(sys.argv, ['JOB_NAME'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Constants
base_api_url = "https://developer.usastaffing.gov/"
api_key = '6WsWnZc1h11900R1JJEsNrVbQoCXyX5d11qvEmzbbvyROFadle08EYsAÂ¡IsmVWyr'
headers = {'Api-Key': api_key}

# UDF to extract IDs from links
@udf(returnType=StringType())
def extract_id_from_links(links, rel_value):
    for link in links:
        if link['rel'] == rel_value:
            try:
                if ('/by/' in link['href']):
                    val = link['href'].split('/')[-1]
                    return val
                else:
                    val = link['href'].split('/')[3]
                    return val
            except Exception as e:
                print(link['href'])
                print(e)
    return None

class ReadAPI:
    def __init__(self, table_name, api_url_key, table_schema, timestamp_col_li):
        self.headers = headers
        self.table_schema = table_schema
        self.timestamp_columns = timestamp_col_li
        self.api_url = base_api_url + "/api/" + api_url_key
        self.table_name = table_name
        self.currentpage = self.load_last_completed_page()

    def load_last_completed_page(self):
        if os.path.exists("last_page.json"):
            with open("last_page.json", "r") as file:
                data = json.load(file)
                return data.get(self.table_name, 1)  # Default to 1 if not found
        return 1  # Default to 1 if file does not exist

    def save_last_completed_page(self):
        if os.path.exists("last_page.json"):
            with open("last_page.json", "r") as file:
                data = json.load(file)
        else:
            data = {}

        data[self.table_name] = self.currentpage

        with open("last_page.json", "w") as file:
            json.dump(data, file)

    def convert_raw_list_to_df(self, raw_api_data_list):
        df = spark.createDataFrame(raw_api_data_list, schema=self.table_schema)
        return df

    def fetch_api_data(self, api_url):
        try:
            response = requests.get(api_url, headers=self.headers)
            if response.status_code == 200:
                return response.json()
        except Exception as e:
            print(e)
        return None

    def handle_pagination(self, raw_api_data):
        page_dict = raw_api_data['paging']
        self.totalcount = page_dict['metadata']['totalCount']
        self.pagesize = page_dict['metadata']['pageSize']
        self.currentpage = page_dict['metadata']['currentPage']
        self.totalpages = page_dict['metadata']['totalPages']
        self.snapshot = page_dict['metadata']['snapshot']

    def extract_links_rel_list(self, raw_api_data):
        link_li = raw_api_data['data'][0]['_links']
        self.links_rel_list = [ele['rel'] for ele in link_li if ele['rel'] != 'self']

    def concat_df(self, new_df):
        self.df = self.df.union(new_df)

    def handle_links(self):
        df = self.df
        for rel_value in self.links_rel_list:
            df = df.drop(rel_value)
        self.df = df

    def handle_timestamp_columns(self): 
        for column in self.timestamp_columns:
            self.df = self.df.withColumn(column, when(~col(column).contains('.'), concat(col(column), lit('.000'))).otherwise(col(column)))
            self.df = self.df.withColumn(column, to_timestamp(col(column), "yyyy-MM-dd'T'HH:mm:ss.SSSSSSSSS"))

    def write_to_redshift(self):
        self.handle_links()
        self.handle_timestamp_columns()
        self.df = self.df.drop('_links')
        input_dynamic_frame = DynamicFrame.fromDF(self.df, glueContext, "input_dynamic_frame")
        glueContext.write_dynamic_frame.from_jdbc_conf(
            frame=input_dynamic_frame,
            catalog_connection="Redshift connection hod-dev-db",
            connection_options=my_conn_options,
            redshift_tmp_dir="s3://aws-glue-assets-094737541415-us-gov-west-1/temporary/"
        )
        self.save_last_completed_page()  # Save last completed page after writing

    def process_next_df(self):
        raw_api_data = self.fetch_api_data(base_api_url + self.next_api_url)
        if raw_api_data:
            self.handle_pagination(raw_api_data)
            raw_api_data_list = raw_api_data['data']
            new_df = self.convert_raw_list_to_df(raw_api_data_list)
            self.concat_df(new_df)

    def mark_checkpoint(self):
        self.df = spark.createDataFrame([], schema=self.table_schema)

    def mark_beginning(self):
        print(f"Processing {self.table_name} - totalCount: {self.totalcount}, pageSize: {self.pagesize}, totalPages: {self.totalpages}")

    def execute(self):
        raw_api_data = self.fetch_api_data(self.api_url)
        self.extract_links_rel_list(raw_api_data)
        if raw_api_data:
            self.handle_pagination(raw_api_data)
            self.mark_beginning()
            raw_api_data_list = raw_api_data['data']
            self.df = self.convert_raw_list_to_df(raw_api_data_list)
            if self.totalpages == 1:
                self.write_to_redshift()
            for i in range(2, self.totalpages + 1):
                if i >= self.currentpage:  # Start from the last completed page
                    self.process_next_df()
                    self.write_to_redshift()
                    self.mark_checkpoint()
                    if i == self.totalpages:
                        print("Completed the process.")

# Process Tables
table_dict = {
    1: "Announcement",
    2: "Application",
    3: "Assessment",
    4: "CertificateApplication",
    5: "Certificate",
    6: "Customer",
    7: "NewHire",
    8: "NewHireAppointingAuthority",
    9: "Office",
    10: "OnboardingTask",
    11: "Organization",
    12: "RequestAppointingAuthority",
    13: "Request",
    14: "Review",
    15: "StaffingTask",
    16: "TimeToHire",
    17: "VacancyAppointingAuthority",
    18: "VacancyFlag",
    19: "Vacancy"
}

process_tables = [8]
batch_size = 10

for i in process_tables:
    table_name = table_dict[i]
    redshift_table_name = f"prakash_csv_Test.{table_name}"
    my_conn_options = {
        "dbtable": redshift_table_name,
        "database": "hcd-dev-db"
    }

    table_schema, timestamp_col_li, api_url_key = mp.get_table_attributes(table_name)
    api_reader = ReadAPI(redshift_table_name, api_url_key, table_schema, timestamp_col_li)
    df = api_reader.execute()
